{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a84886e6",
   "metadata": {},
   "source": [
    "# SIMULATING DATA BASED ON STATISTICAL DISTRIBUTION/ PROPERTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50faff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the adult income dataset\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header=None)\n",
    "\n",
    "# Define column names\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income']\n",
    "\n",
    "# Assign column names to dataframe\n",
    "df.columns = columns\n",
    "\n",
    "# Subset the data to include only the columns we are interested in\n",
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income']\n",
    "df = df[cols]\n",
    "\n",
    "# Calculate the means and standard deviations of the numerical columns\n",
    "num_cols = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "means = df[num_cols].mean()\n",
    "stds = df[num_cols].std()\n",
    "\n",
    "# Calculate the minority class size and majority class size based on 5% and 95% ratio respectively\n",
    "minority_class_size = 0.05 * 20000\n",
    "majority_class_size = 20000 - minority_class_size\n",
    "\n",
    "# Simulate minority class\n",
    "minority_data = pd.DataFrame()\n",
    "for col in df.columns:\n",
    "    if col in num_cols:\n",
    "        minority_data[col] = np.random.normal(means[col], stds[col], int(minority_class_size))\n",
    "    else:\n",
    "        minority_data[col] = np.random.choice(df[col].unique(), int(minority_class_size))\n",
    "\n",
    "minority_data['income_binary'] = 1\n",
    "\n",
    "# Simulate majority class\n",
    "majority_data = pd.DataFrame()\n",
    "for col in df.columns:\n",
    "    if col in num_cols:\n",
    "        majority_data[col] = np.random.normal(means[col], stds[col], int(majority_class_size))\n",
    "    else:\n",
    "        majority_data[col] = np.random.choice(df[col].unique(), int(majority_class_size))\n",
    "\n",
    "majority_data['income_binary'] = 0\n",
    "\n",
    "# Concatenate minority and majority data to get the final dataset\n",
    "sim_data = pd.concat([minority_data, majority_data], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows of the dataset\n",
    "sim_data = sim_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the simulated dataset as a CSV file\n",
    "sim_data.to_csv('simulated_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd46ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b219bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Subset the data to include only the columns we are interested in\n",
    "cols = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income', 'income_binary']\n",
    "df = df[cols]\n",
    "\n",
    "# Drop the 'income' column\n",
    "df.drop('income', axis=1, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isna().sum())\n",
    "\n",
    "\n",
    "\n",
    "# Convert the 'income_binary' column to integer type\n",
    "df['income_binary'] = df['income_binary'].astype(int)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d199ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Splitting the data into X and y\n",
    "X = df.drop('income_binary', axis=1)\n",
    "y = df['income_binary']\n",
    "\n",
    "# Applying Random Oversampling\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "\n",
    "# Counting the values of both classes after oversampling\n",
    "y_over.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a5b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b14183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Splitting the data into X and y\n",
    "X = df.drop('income_binary', axis=1)\n",
    "y = df['income_binary']\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']),\n",
    "        ('cat', categorical_transformer, ['workclass', 'education', 'marital.status', 'occupation', \n",
    "                                          'relationship', 'race', 'sex', 'native.country'])])\n",
    "# Defining the pipelines for each algorithm\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('clf', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Fitting the pipelines\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the models\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_lr))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_lr))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_lr))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\nSupport Vector Machine:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_svm))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_svm))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_svm))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ca991",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping Data Basis features\n",
    "\n",
    "cms_patient_data_cnt = cms_patient_data[['BeneID', 'ClaimID']].groupby(cms_patient_data['Provider']).nunique().reset_index()\n",
    "cms_patient_data_cnt.rename(columns={'BeneID':'BeneID_count','ClaimID':'ClaimID_count'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756580d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef7f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ac5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Define your tabular data as a Pandas DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35],\n",
    "    'salary': [50000, 60000, 70000]\n",
    "})\n",
    "\n",
    "# Generate a key\n",
    "key = Fernet.generate_key()\n",
    "\n",
    "# Create a Fernet object with the key\n",
    "cipher_suite = Fernet(key)\n",
    "\n",
    "# Encrypt the data\n",
    "encrypted_data = {}\n",
    "for column_name in data.columns:\n",
    "    column_data = data[column_name].astype(str).tolist()\n",
    "    encrypted_column_data = [cipher_suite.encrypt(str.encode(value)) for value in column_data]\n",
    "    encrypted_data[column_name] = encrypted_column_data\n",
    "\n",
    "# Store the encrypted data\n",
    "encrypted_data_df = pd.DataFrame(encrypted_data)\n",
    "\n",
    "# Decrypt the data\n",
    "decrypted_data = {}\n",
    "for column_name in encrypted_data_df.columns:\n",
    "    encrypted_column_data = encrypted_data_df[column_name].tolist()\n",
    "    decrypted_column_data = [cipher_suite.decrypt(value).decode('utf-8') for value in encrypted_column_data]\n",
    "    decrypted_data[column_name] = decrypted_column_data\n",
    "\n",
    "# Store the decrypted data\n",
    "decrypted_data_df = pd.DataFrame(decrypted_data)\n",
    "\n",
    "print(decrypted_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23da80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Generate a key\n",
    "key = Fernet.generate_key()\n",
    "\n",
    "# Create a Fernet object with the key\n",
    "cipher_suite = Fernet(key)\n",
    "\n",
    "# Define a function to encrypt a DataFrame\n",
    "def encrypt_dataframe(df, key):\n",
    "    # Convert the DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    # Encrypt the CSV string\n",
    "    encrypted_csv_string = cipher_suite.encrypt(csv_string.encode())\n",
    "\n",
    "    # Convert the encrypted CSV string back to a DataFrame\n",
    "    encrypted_df = pd.read_csv(io.StringIO(encrypted_csv_string.decode()))\n",
    "\n",
    "    return encrypted_df\n",
    "\n",
    "# Define a function to decrypt a DataFrame\n",
    "def decrypt_dataframe(df, key):\n",
    "    # Convert the DataFrame to a CSV string\n",
    "    csv_string = df.to_csv(index=False)\n",
    "\n",
    "    # Decrypt the CSV string\n",
    "    decrypted_csv_string = cipher_suite.decrypt(csv_string.encode())\n",
    "\n",
    "    # Convert the decrypted CSV string back to a DataFrame\n",
    "    decrypted_df = pd.read_csv(io.StringIO(decrypted_csv_string.decode()))\n",
    "\n",
    "    return decrypted_df\n",
    "\n",
    "# Example usage\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': ['foo', 'bar', 'baz']})\n",
    "\n",
    "# Encrypt the DataFrame\n",
    "encrypted_df = encrypt_dataframe(df, key)\n",
    "print(encrypted_df)\n",
    "\n",
    "# Decrypt the DataFrame\n",
    "decrypted_df = decrypt_dataframe(encrypted_df, key)\n",
    "print(decrypted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406aed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt  \n",
    "from pandas import DataFrame, Series\n",
    "import datetime as DT\n",
    "import json\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report,confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/p_adi/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('relpase_data.csv')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ce4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_part(df_col):\n",
    "    # regular expression to extract numeric part\n",
    "    pattern = re.compile(r'\\s*(\\d+)\\s*')\n",
    "\n",
    "    # iterate over the column and extract numeric part\n",
    "    numeric_part = []\n",
    "    for val in df_col:\n",
    "        if isinstance(val, str):\n",
    "            match = pattern.search(val)\n",
    "            if match:\n",
    "                numeric_part.append(int(match.group(1)))\n",
    "            else:\n",
    "                numeric_part.append(None)\n",
    "        elif isinstance(val, int):\n",
    "            numeric_part.append(val)\n",
    "        else:\n",
    "            numeric_part.append(None)\n",
    "\n",
    "    # return the new column as a pandas Series\n",
    "    return pd.Series(numeric_part)\n",
    "\n",
    "data1['weight'] = extract_numeric_part(data1['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77488ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d312a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['care_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['multiple_occurences'].value_counts()\n",
    "\n",
    "\n",
    "## What is the meaning of different LOS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var_counts = data1['multiple_occurences'].value_counts()\n",
    "cat_var_percentages = cat_var_counts / len(data1) * 100\n",
    "\n",
    "# Create a horizontal bar chart with counts and percentages\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(cat_var_counts.index, cat_var_counts.values)\n",
    "ax.set_xlabel('Count')\n",
    "ax2 = ax.twiny()\n",
    "ax2.barh(cat_var_percentages.index, cat_var_percentages.values, alpha=0.4, color='g')\n",
    "ax2.set_xlabel('Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc817ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70336909",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461998ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c47a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f3257",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a figure and axes objects\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n",
    "\n",
    "# plot a boxplot of the data on the first axis\n",
    "sns.boxplot(data=data1, x='weight', ax=axs[0])\n",
    "axs[0].set_title('Box plot of weight')\n",
    "\n",
    "# plot a histogram of the data on the second axis\n",
    "sns.histplot(data=data1, x='weight', bins=50, ax=axs[1])\n",
    "axs[1].set_title('Histogram of weight')\n",
    "\n",
    "# adjust the layout and show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a figure and axes objects\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n",
    "\n",
    "# plot a boxplot of the data on the first axis\n",
    "sns.boxplot(data=data1, x='weight', ax=axs[0])\n",
    "axs[0].set_title('Box plot of weight')\n",
    "\n",
    "# plot a histogram of the data on the second axis\n",
    "sns.histplot(data=data1, x='weight', bins=range(1000, 20001, 1000), ax=axs[1])\n",
    "axs[1].set_title('Histogram of weight')\n",
    "\n",
    "# adjust the layout and show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create a figure and axes objects\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n",
    "\n",
    "# plot a boxplot of the data on the first axis\n",
    "sns.boxplot(data=data1, x='weight', ax=axs[0])\n",
    "axs[0].set_title('Box plot of weight')\n",
    "\n",
    "# plot a distplot of the data after logarithmic transformation on the second axis\n",
    "sns.distplot(np.log(data1['weight']), bins=range(0, 10), ax=axs[1])\n",
    "axs[1].set_title('Distribution plot of log(weight)')\n",
    "\n",
    "# adjust the layout and show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa17779",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['target'] = data1['multiple_occurences'].apply(lambda x : 1 if x == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22610f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_conv_features= ['cpt_code', 'rev_code', 'diagcode', 'care_level', 'label']\n",
    "\n",
    "ohe_features = ['cpt_code', 'rev_code', 'diagcode', 'care_level', 'label']\n",
    "\n",
    "le_features = ['cpt_code', 'rev_code', 'diagcode', 'care_level', 'label']\n",
    "\n",
    "scaling_features = ['weight', 'height', 'location_los']\n",
    "\n",
    "percent_value = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_str(df,conv_features=[]): \n",
    "    for feature in conv_features:\n",
    "        df[feature]= df[feature].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b525a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_engineered_features(df,engineered_features):\n",
    "    for ef, fun in engineered_features.items():\n",
    "        method_apply = fun\n",
    "        df = method_apply(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaler(in_df, feature=[]):\n",
    "    #for feature in col_list:\n",
    "        scaler = StandardScaler()\n",
    "        scaling_cols = in_df[feature]\n",
    "        scaler.fit(scaling_cols.values)\n",
    "        scaling_cols = scaler.transform(scaling_cols.values)\n",
    "        in_df[feature] = scaling_cols\n",
    "    # scaled = scaler.fit_transform(col_df).toarray()\n",
    "        print('#'*10, f'scaled: {feature}', '#'*10)\n",
    "        return in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent_row_class(df, feature, percent_value):\n",
    "    percentage_row_count=df[feature].value_counts(normalize=True).mul(100)\n",
    "    cumsum_percentage_count = percentage_row_count.cumsum()\n",
    "    d = cumsum_percentage_count[cumsum_percentage_count<=percent_value]\n",
    "    if len(d)!=0 and d[len(d)-1]<percent_value:\n",
    "        length = len(d)\n",
    "        m = cumsum_percentage_count[cumsum_percentage_count==cumsum_percentage_count[length]]\n",
    "        category_list =  list(d.index)+ list(m.index)\n",
    "    elif len(d)== 0 :\n",
    "        m = cumsum_percentage_count[cumsum_percentage_count==cumsum_percentage_count[0]]\n",
    "        category_list =  list(m.index)\n",
    "    else:\n",
    "        category_list =  list(d.index)\n",
    "    return category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed513c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder(in_df,col_name_with_threshold=[], percent_value= percent_value):\n",
    "\n",
    "    label_map = {}\n",
    "    for col_name in col_name_with_threshold:\n",
    "        val_cnt_df = in_df[col_name].value_counts()\n",
    "        # if count_threshold != 0:\n",
    "        #     series = pd.value_counts(in_df[col_name])\n",
    "        #     mask = series.lt(count_threshold)\n",
    "        in_df[col_name] = np.where(in_df[col_name].isin(\n",
    "        calculate_percent_row_class(in_df,col_name,percent_value)),  in_df[col_name], 'Other')\n",
    "        \n",
    "            \n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(in_df[col_name])\n",
    "        in_df[col_name] = encoder.transform(in_df[col_name])\n",
    "        \n",
    "        label_codes = np.arange(0,len(encoder.classes_),1)\n",
    "        \n",
    "        # a = encoder.inverse_transform(label_codes)\n",
    "        # print(a)\n",
    "        le_map = dict(zip(encoder.inverse_transform(label_codes),label_codes))\n",
    "        # print(le_map)\n",
    "        # pd.DataFrame.from_dict({col_name: list(a), col_name+'_map':list(label_codes)}).to_csv(col_name+'.csv')\n",
    "        \n",
    "        label_map[col_name] = le_map\n",
    "\n",
    "        print('#'*10, f'label encoded: {col_name}', '#'*10)\n",
    "\n",
    "    return label_map, in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_deniedApproved_model(df, le_features, ohe_features, scaling_features, dtype_conv_features):\n",
    "    Label_map = {}\n",
    "    df = set_str(df, dtype_conv_features)\n",
    "\n",
    "    # df,dc_map = set_denialcode_as_target(df)\n",
    "    # df = feature_scaler(df,scaling_features)\n",
    "    Label_map, df = label_encoder(df, le_features)\n",
    "    #df = ohe_encoder(df, ohe_features)\n",
    "    # for i in ohe_features:\n",
    "    #     encoded_df = ohe_encoder(df, i)\n",
    "    #     df.drop([i],axis=1,inplace=True)\n",
    "    #     df = df.merge(encoded_df,left_index=True,right_index=True)\n",
    "    #     print(f\"After ohe of {i} shape:\", df.shape)\n",
    "    out_df = pd.get_dummies(df, columns = ohe_features)\n",
    "    print(\" shape:\", out_df.shape)\n",
    "    return out_df,Label_map\n",
    "    # return df\n",
    "    \n",
    "df_combined,Label_map = preprocessing_deniedApproved_model(data1, le_features, ohe_features,  scaling_features, dtype_conv_features)\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c2f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.distplot(df_combined['height'], ax=ax)\n",
    "\n",
    "# Set the size of the plot\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.boxplot(df_combined['height'], ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.distplot(df_combined['location_los'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a631c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.boxplot(df_combined['location_los'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create a DataFrame with numeric data\n",
    "\n",
    "\n",
    "# Create a Q-Q plot of the data\n",
    "stats.probplot(df_combined['location_los'], dist='norm', plot=plt)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create a DataFrame with numeric data\n",
    "\n",
    "\n",
    "# Create a Q-Q plot of the data\n",
    "stats.probplot(df_combined['weight'], dist='norm', plot=plt)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create a DataFrame with numeric data\n",
    "\n",
    "\n",
    "# Create a Q-Q plot of the data\n",
    "stats.probplot(df_combined['height'], dist='norm', plot=plt)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['patientmasterkey','multiple_occurences'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# # Define column transformers\n",
    "# numeric_transformer = StandardScaler()\n",
    "# categorical_transformer = OneHotEncoder(drop=\"first\")\n",
    "\n",
    "# # Encode categorical variables and scale numeric variables\n",
    "# ct = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", numeric_transformer, ['weight', 'height', 'location_los']),\n",
    "#         (\"cat\", categorical_transformer, ['cpt_code', 'rev_code', 'diagcode', 'care_level', 'label'])\n",
    "#     ])\n",
    "\n",
    "# # Transform the data\n",
    "# X = data1.drop(\"target\", axis=1)\n",
    "# y = data1[\"target\"]\n",
    "# X = ct.fit_transform(X)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# # Train the Random Forest Classifier using Grid Search\n",
    "# param_grid = {\n",
    "#     \"n_estimators\": [100, 300],\n",
    "#     \"max_depth\": [5, 10, 20],\n",
    "#     \"min_samples_split\": [2, 5],\n",
    "#     \"min_samples_leaf\": [1, 2, 4, 6],\n",
    "#     \"max_features\": [\"sqrt\"]\n",
    "# }\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and the best score obtained\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# # Evaluate the model on the testing data\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feea679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Define column transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop=\"first\")\n",
    "\n",
    "# Encode categorical variables and scale numeric variables\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, ['weight', 'height', 'location_los']),\n",
    "        (\"cat\", categorical_transformer, ['cpt_code', 'rev_code', 'diagcode', 'care_level', 'label'])\n",
    "    ])\n",
    "\n",
    "# Split the data into X and y\n",
    "X = data1.drop(\"target\", axis=1)\n",
    "y = data1[\"target\"]\n",
    "\n",
    "# Transform the data\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier using Grid Search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 300],\n",
    "    \"max_depth\": [5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6],\n",
    "    \"max_features\": [\"sqrt\"]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the best score obtained\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1\n",
    "data2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdacc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop=\"first\")\n",
    "\n",
    "# Encode categorical variables and scale numeric variables\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, ['weight', 'height']),\n",
    "        (\"cat\", categorical_transformer, ['cpt_code',  'diagcode', 'care_level', 'label'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92982651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = data2.drop(\"target\", axis=1)\n",
    "y = data2[\"target\"]\n",
    "\n",
    "# Transform the data\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036412c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Random Forest Classifier using Grid Search\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 300],\n",
    "    \"max_depth\": [5, 10, 20],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6],\n",
    "    \"max_features\": [\"sqrt\"]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e37e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters and the best score obtained\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056c180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700c54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d6110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae82b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "from numpy import percentile\n",
    "\n",
    "def outlier_treatment(datacolumn):\n",
    "    \n",
    "    q25, q75, q50 = percentile(datacolumn, 25), percentile(datacolumn, 75), percentile(datacolumn, 50)\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    # calculate the outlier cutoff\n",
    "    # cut_off = iqr * 1.5\n",
    "    # extreme outliers - >\n",
    "    cut_off = iqr * 3\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    return iqr, lower, upper, q25, q75, q50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679653c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treat  for outliers - \n",
    "def checkOutlier(data, checkOutlierCol):\n",
    "        # for count in range(0,len(checkOutlierCol)) :\n",
    "            # print(count)\n",
    "            iqr, lower, upper, q25, q75, q50 = outlier_treatment(data[checkOutlierCol])\n",
    "\n",
    "            print(checkOutlierCol, \" : \", iqr)\n",
    "            print(checkOutlierCol, \" : \",lower)\n",
    "            print(checkOutlierCol, \" : \",upper)\n",
    "\n",
    "            Upper_outliers =data.loc[data[checkOutlierCol] > upper]\n",
    "            above_upper_index =data.loc[data[checkOutlierCol] > upper].index\n",
    "            Lower_outliers =data.loc[data[checkOutlierCol] < lower]\n",
    "            print(checkOutlierCol, \": Number of rows present above upper cutoff : \", Upper_outliers.shape)\n",
    "            print(checkOutlierCol, \": Number of rows present below lower cutoff : \",Lower_outliers.shape)\n",
    "            print(checkOutlierCol , ': Percentiles: 25th=%.3f, 75th=%.3f, 50th=%.3f, IQR=%.3f' % (q25, q75, q50, iqr))\n",
    "            return above_upper_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036fb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = checkOutlier(df_combined,'height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d17b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = checkOutlier(df_combined,'location_los')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in Label_map.items():\n",
    "    if 'Other' not in Label_map[i].keys():\n",
    "        ot = max(Label_map[i].values())+1\n",
    "        Label_map[i]['Other']= ot\n",
    "        l = i+'_'+str(ot)\n",
    "        df_combined = df_combined.assign(**dict.fromkeys([l], 0))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932741d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Custom encoder for numpy data types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "                            np.int16, np.int32, np.int64, np.uint8,\n",
    "                            np.uint16, np.uint32, np.uint64)):\n",
    "\n",
    "            return int(obj)\n",
    "\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "\n",
    "        elif isinstance(obj, (np.complex_, np.complex64, np.complex128)):\n",
    "            return {'real': obj.real, 'imag': obj.imag}\n",
    "\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "\n",
    "        elif isinstance(obj, (np.bool_)):\n",
    "            return bool(obj)\n",
    "\n",
    "        elif isinstance(obj, (np.void)): \n",
    "            return None\n",
    "\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Label_map.json', 'w') as file:\n",
    "    json.dump(Label_map, file, indent=4, sort_keys=True,\n",
    "              separators=(', ', ': '), ensure_ascii=False,\n",
    "              cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_processed_data_no_duplicate = df_combined.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(df_final_processed_data_no_duplicate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_remove =['patientmasterkey','cpt_code', 'rev_code', 'diagcode', 'care_level','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ls_remove:\n",
    "    ls.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363aeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSize = 0.2\n",
    "seed = 7\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_sampled_data.drop(['target'],axis=1)\n",
    "y = pd.DataFrame(df_sampled_data['target'],columns=['target'],index=x.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65895ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2422f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('C:/Users/p_adi/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b8f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('data_ts.csv')\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "data1['Date'] = pd.to_datetime(data1['Departure_Date'])\n",
    "df_filtered = data1[(data1['Date'].dt.year == 2022)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1641e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221ab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d957bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the time series to visualize its trends and patterns\n",
    "data1['Total_Available_Seats'].plot(figsize=(12,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered[['Date', 'MARKET', 'Total_Available_Seats', 'Cum_Customer_Segment_Count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f795621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffbd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Available_Seats'].interpolate(method ='linear', limit_direction ='backward', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the date column to a pandas datetime object\n",
    "# df['Departure_Date'] = pd.to_datetime(df['Departure_Date'])\n",
    "\n",
    "# # Set the date column as the index of the dataframe\n",
    "# df.set_index('Departure_Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5766a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the date column to a pandas datetime object\n",
    "# df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# # Set the date column as the index of the dataframe\n",
    "# df.set_index('Date', inplace=True)\n",
    "# df.index = pd.to_datetime(df['Date'].index)\n",
    "# df.index.freq = pd.infer_freq(df.index)\n",
    "# df_resampled = df.resample('D').sum()\n",
    "\n",
    "\n",
    "\n",
    "# convert the date column to a datetime object\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# set the date column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "# # create a DatetimeIndex with a daily frequency\n",
    "# idx = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "\n",
    "# # create a new dataframe with the missing dates filled in\n",
    "# df_resampled = df.reindex(idx)\n",
    "\n",
    "# create a PeriodIndex with a daily frequency\n",
    "df_resampled = df_resampled.to_period('D')\n",
    "\n",
    "# # Convert the index to a PeriodIndex\n",
    "# df.index = pd.PeriodIndex(df['Departure_Datetime_Local'].index, freq='D')\n",
    "\n",
    "# # Resample the data using a PeriodIndex with a valid frequency set\n",
    "# df_resampled = df.resample('D').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1bbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03521e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "new_index = pd.date_range(start=df.index.min(), end=df.index.max())\n",
    "# # reindex the DataFrame with the new index, filling missing values with NaN\n",
    "df = df.reindex(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95750fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.reindex(new_index, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Total_Available_Seats'])\n",
    "df = df.dropna(subset=['MARKET'])\n",
    "df = df.dropna(subset=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e170dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series into its components using an additive model\n",
    "result_add = seasonal_decompose(df['Cum_Customer_Segment_Count'], model='additive')\n",
    "result_add.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21869a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c6f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34356e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Decompose the time series into its components using a multiplicative model\n",
    "result_mult = seasonal_decompose(df, model='multiplicative')\n",
    "result_mult.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Fit an Exponential Smoothing model using an additive trend and additive seasonal component\n",
    "model_add = ExponentialSmoothing(df, trend='add', seasonal='add').fit()\n",
    "\n",
    "# Fit an Exponential Smoothing model using a multiplicative trend and multiplicative seasonal component\n",
    "model_mult = ExponentialSmoothing(df, trend='mul', seasonal='mul').fit()\n",
    "\n",
    "# Generate forecasted values for the next 30 days using the additive model\n",
    "forecast_add = model_add.forecast(steps=30)\n",
    "\n",
    "# Generate forecasted values for the next 30 days using the multiplicative model\n",
    "forecast_mult = model_mult.forecast(steps=30)\n",
    "\n",
    "# Plot the forecasted values against the actual values\n",
    "plt.plot(df.index, df, label='Actual')\n",
    "plt.plot(forecast_add.index, forecast_add, label='Additive Forecast')\n",
    "plt.plot(forecast_mult.index, forecast_mult, label='Multiplicative Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d3f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googleapis_common_protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a84dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install googleapis_common_protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab6b4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461e93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30836499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71c64bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "import os\n",
    "os.chdir('C:/Users/p_adi/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8c7ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p_adi\\AppData\\Local\\Temp\\ipykernel_10324\\2348881270.py:1: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('relapse_faiss.csv')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('relapse_faiss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fce13961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientmasterkey</th>\n",
       "      <th>cpt_code</th>\n",
       "      <th>rev_code</th>\n",
       "      <th>diagcode</th>\n",
       "      <th>gender</th>\n",
       "      <th>dob</th>\n",
       "      <th>paymentmethod</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>comorbidity</th>\n",
       "      <th>care_level</th>\n",
       "      <th>admission_date</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>location_los</th>\n",
       "      <th>program_los</th>\n",
       "      <th>levelofcare_los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1028_1093</td>\n",
       "      <td>S9475,90837,INTEREST,H0018</td>\n",
       "      <td>0906,0912,INT,1002,0913,0126</td>\n",
       "      <td>F33.1,F33.1,F41.1,F12.90,,,,,,,</td>\n",
       "      <td>Female</td>\n",
       "      <td>1994-01-13</td>\n",
       "      <td>Insurance Tx and Labs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IOP,Ancillary,PHP,Custom,RTC,Detox</td>\n",
       "      <td>2020-08-03 15:00:00</td>\n",
       "      <td>severe</td>\n",
       "      <td>215</td>\n",
       "      <td>67</td>\n",
       "      <td>48</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028_1138</td>\n",
       "      <td>H0018</td>\n",
       "      <td>1002,0128,0126</td>\n",
       "      <td>F43.10,F43.10,F41.1,F10.20,F10.929,F51.4,F12.1...</td>\n",
       "      <td>Male</td>\n",
       "      <td>1966-01-19</td>\n",
       "      <td>Insurance Tx and Labs</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RTC,Detox</td>\n",
       "      <td>2020-09-05 11:00:00</td>\n",
       "      <td>less_severe</td>\n",
       "      <td>279.9</td>\n",
       "      <td>75</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1028_1222</td>\n",
       "      <td>H0018</td>\n",
       "      <td>1002,0126</td>\n",
       "      <td>F12.20,F12.20,F41.1,F40.10,M10.071,F17.210,,,,,</td>\n",
       "      <td>Male</td>\n",
       "      <td>1972-08-31</td>\n",
       "      <td>Insurance Tx and Labs</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RTC,Detox</td>\n",
       "      <td>2020-10-19 16:30:00</td>\n",
       "      <td>other</td>\n",
       "      <td>168</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1028_1228</td>\n",
       "      <td>S9475,H0018</td>\n",
       "      <td>0913,1002,0912</td>\n",
       "      <td>F40.10,F40.10,F10.20,,,,,,,,</td>\n",
       "      <td>Male</td>\n",
       "      <td>1983-09-06</td>\n",
       "      <td>Insurance Tx and Labs</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHP,RTC</td>\n",
       "      <td>2020-10-29 17:00:00</td>\n",
       "      <td>less_severe</td>\n",
       "      <td>174.4</td>\n",
       "      <td>71</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1028_1241</td>\n",
       "      <td>S0201,H0018</td>\n",
       "      <td>0913,0126,1002</td>\n",
       "      <td>F10.239,F10.239,F17.208,,,,,,,,</td>\n",
       "      <td>Male</td>\n",
       "      <td>1975-10-24</td>\n",
       "      <td>Insurance Tx and Labs</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PHP,Detox,RTC</td>\n",
       "      <td>2020-11-06 17:00:00</td>\n",
       "      <td>other</td>\n",
       "      <td>151</td>\n",
       "      <td>73</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patientmasterkey                    cpt_code                      rev_code   \n",
       "0        1028_1093  S9475,90837,INTEREST,H0018  0906,0912,INT,1002,0913,0126  \\\n",
       "1        1028_1138                       H0018                1002,0128,0126   \n",
       "2        1028_1222                       H0018                     1002,0126   \n",
       "3        1028_1228                 S9475,H0018                0913,1002,0912   \n",
       "4        1028_1241                 S0201,H0018                0913,0126,1002   \n",
       "\n",
       "                                            diagcode  gender         dob   \n",
       "0                    F33.1,F33.1,F41.1,F12.90,,,,,,,  Female  1994-01-13  \\\n",
       "1  F43.10,F43.10,F41.1,F10.20,F10.929,F51.4,F12.1...    Male  1966-01-19   \n",
       "2    F12.20,F12.20,F41.1,F40.10,M10.071,F17.210,,,,,    Male  1972-08-31   \n",
       "3                       F40.10,F40.10,F10.20,,,,,,,,    Male  1983-09-06   \n",
       "4                    F10.239,F10.239,F17.208,,,,,,,,    Male  1975-10-24   \n",
       "\n",
       "           paymentmethod ethnicity comorbidity   \n",
       "0  Insurance Tx and Labs       NaN         NaN  \\\n",
       "1  Insurance Tx and Labs     Other         NaN   \n",
       "2  Insurance Tx and Labs     Other         NaN   \n",
       "3  Insurance Tx and Labs     Other         NaN   \n",
       "4  Insurance Tx and Labs     Other         NaN   \n",
       "\n",
       "                           care_level       admission_date        label   \n",
       "0  IOP,Ancillary,PHP,Custom,RTC,Detox  2020-08-03 15:00:00       severe  \\\n",
       "1                           RTC,Detox  2020-09-05 11:00:00  less_severe   \n",
       "2                           RTC,Detox  2020-10-19 16:30:00        other   \n",
       "3                             PHP,RTC  2020-10-29 17:00:00  less_severe   \n",
       "4                       PHP,Detox,RTC  2020-11-06 17:00:00        other   \n",
       "\n",
       "  weight  height  location_los  program_los  levelofcare_los  \n",
       "0    215      67            48           23               19  \n",
       "1  279.9      75            19           13               12  \n",
       "2    168      69            10            5                5  \n",
       "3  174.4      71            16            8                8  \n",
       "4    151      73            16           10                8  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1117b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_column_name = ['patientmasterkey', 'cpt_code', 'gender', 'dob', 'paymentmethod', 'ethnicity', 'comorbidity', 'care_level', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "727893d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=categorical_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e340baa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97843 entries, 0 to 97842\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   patientmasterkey  97843 non-null  object\n",
      " 1   cpt_code          83989 non-null  object\n",
      " 2   rev_code          58812 non-null  object\n",
      " 3   diagcode          97843 non-null  object\n",
      " 4   gender            97711 non-null  object\n",
      " 5   dob               97843 non-null  object\n",
      " 6   paymentmethod     97636 non-null  object\n",
      " 7   ethnicity         46745 non-null  object\n",
      " 8   comorbidity       5794 non-null   object\n",
      " 9   care_level        97843 non-null  object\n",
      " 10  admission_date    97843 non-null  object\n",
      " 11  label             97843 non-null  object\n",
      " 12  weight            97843 non-null  object\n",
      " 13  height            97843 non-null  int64 \n",
      " 14  location_los      97843 non-null  int64 \n",
      " 15  program_los       97843 non-null  int64 \n",
      " 16  levelofcare_los   97843 non-null  int64 \n",
      "dtypes: int64(4), object(13)\n",
      "memory usage: 12.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3dee2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7732424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocess_data function\n",
    "def preprocess_data(p_keys, cpt_codes, gender, ethnicity, location_ids, c_level, target):\n",
    "    # Implement your data preprocessing logic here\n",
    "    # This is just a dummy function, replace with your actual data preprocessing logic\n",
    "    return [p_keys, cpt_codes, gender, ethnicity, location_ids, c_level, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e141bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Extract relevant columns from the DataFrame\n",
    "# Assuming your CSV file has columns 'medical_history', 'diagnoses', 'treatments', and 'target' containing the relevant data\n",
    "\n",
    "p_keys = data['patientmasterkey'].tolist()\n",
    "cpt_codes = data['cpt_code'].tolist()\n",
    "gender = data['gender'].tolist()\n",
    "ethnicity  = data['ethnicity'].tolist()\n",
    "location_ids = data['location_los'].tolist()\n",
    "c_level = data['care_level'].tolist()\n",
    "target = data['label'].tolist()\n",
    "\n",
    "# Preprocess data and create feature representations\n",
    "# Here, let's assume you have a function called 'preprocess_data' that takes raw data and preprocesses it to create feature vectors\n",
    "data1 = []\n",
    "for i in range(len(p_keys)):\n",
    "    feature_vector = preprocess_data(p_keys[i], cpt_codes[i], gender[i], ethnicity[i], location_ids[i], c_level[i], target[i])\n",
    "    data1.append(feature_vector)\n",
    "\n",
    "# Convert data to numpy array\n",
    "data1 = np.array(data1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a507db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1028_1093', 'S9475,90837,INTEREST,H0018', 'Female', ..., '48',\n",
       "        'IOP,Ancillary,PHP,Custom,RTC,Detox', 'severe'],\n",
       "       ['1028_1138', 'H0018', 'Male', ..., '19', 'RTC,Detox',\n",
       "        'less_severe'],\n",
       "       ['1028_1222', 'H0018', 'Male', ..., '10', 'RTC,Detox', 'other'],\n",
       "       ...,\n",
       "       ['947_9729', 'nan', 'Female', ..., '16', 'Detox,RTC', 'other'],\n",
       "       ['947_9737', '80305,S0201,H0035,J2315,96372', 'Male', ..., '18',\n",
       "        'Ancillary,PHP,RTC,Detox', 'other'],\n",
       "       ['947_9828', '96372,S0201,80305,J2315,H0015,87811,90837', 'Male',\n",
       "        ..., '42', 'Ancillary,PHP,Detox,RTC,IOP,TBD,OP', 'other']],\n",
       "      dtype='<U320')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7450b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_keys, cpt_codes, gender, ethnicity, location_ids, c_level, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa8efa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Faiss index\n",
    "d = data1.shape[1] # dimension of feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6d9406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)  # create L2 distance index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4adff97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001F4AABEF2A0> >"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "180e0965",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'S9475,90837,INTEREST,H0018'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\faiss\\class_wrappers.py:229\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    227\u001b[0m n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\n\u001b[1;32m--> 229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_c(n, swig_ptr(x))\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'S9475,90837,INTEREST,H0018'"
     ]
    }
   ],
   "source": [
    "index.add(data1)  # add data to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03c92542",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_p_keys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Query for similar patients\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m query_patient \u001b[38;5;241m=\u001b[39m preprocess_data(\u001b[43mquery_p_keys\u001b[49m, query_cpt_codes, query_gender, query_ethnicity, query_location_ids, query_c_level, query_target)  \u001b[38;5;66;03m# preprocess query patient data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m query_patient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([query_patient])  \u001b[38;5;66;03m# convert to numpy array\u001b[39;00m\n\u001b[0;32m      4\u001b[0m _, similar_patient_indices \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(query_patient, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# search for 5 most similar patients\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'query_p_keys' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Query for similar patients\n",
    "query_patient = preprocess_data(query_p_keys, query_cpt_codes, query_gender, query_ethnicity, query_location_ids, query_c_level, query_target)  # preprocess query patient data\n",
    "query_patient = np.array([query_patient])  # convert to numpy array\n",
    "_, similar_patient_indices = index.search(query_patient, k=5)  # search for 5 most similar patients\n",
    "\n",
    "# Get similar patients' IDs from the original data\n",
    "similar_patient_ids = data.iloc[similar_patient_indices[0]]['patientmasterkey'].tolist()  # assuming you have a 'patient_id' column in your CSV file\n",
    "\n",
    "# Print similar patient IDs\n",
    "print(\"Similar Patients: \", similar_patient_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12fffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install libomp-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee36b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu --no-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537074f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520515e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
